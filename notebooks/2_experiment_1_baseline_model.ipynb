{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a602b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-2.17.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.17.2 (from mlflow)\n",
      "  Downloading mlflow_skinny-2.17.2-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (3.7.5)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (1.24.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (2.0.3)\n",
      "Collecting pyarrow<18,>=4.0.0 (from mlflow)\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (1.3.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (1.10.1)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading sqlalchemy-2.0.40-cp38-cp38-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow) (3.1.6)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow-skinny==2.17.2->mlflow) (8.1.8)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading databricks_sdk-0.50.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow-skinny==2.17.2->mlflow) (8.5.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_api-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<25 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow-skinny==2.17.2->mlflow) (24.2)\n",
      "Collecting protobuf<6,>=3.12.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading protobuf-5.29.4-cp38-cp38-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from mlflow-skinny==2.17.2->mlflow) (6.0.2)\n",
      "Collecting requests<3,>=2.17.3 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (6.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (304)\n",
      "Collecting urllib3>=1.26.0 (from docker<8,>=4.0.0->mlflow)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from Flask<4->mlflow) (3.0.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from Flask<4->mlflow) (1.8.2)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from graphene<4->mlflow) (2.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from matplotlib<4->mlflow) (3.1.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.1.1-cp38-cp38-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==2.17.2->mlflow) (0.4.6)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.2->mlflow) (3.21.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\heart\\anaconda3\\envs\\mlpro\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading wrapt-1.17.2-cp38-cp38-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\n",
      "   ---------------------------------------- 0.0/26.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/26.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/26.7 MB 1.7 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.8/26.7 MB 1.7 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 1.3/26.7 MB 1.8 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 1.6/26.7 MB 1.7 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 2.1/26.7 MB 1.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 2.6/26.7 MB 1.9 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 3.1/26.7 MB 1.9 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 3.7/26.7 MB 2.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 3.9/26.7 MB 2.0 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 4.7/26.7 MB 2.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 5.0/26.7 MB 2.1 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 5.5/26.7 MB 2.1 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 6.0/26.7 MB 2.1 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.6/26.7 MB 2.1 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 7.1/26.7 MB 2.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 7.6/26.7 MB 2.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 8.1/26.7 MB 2.2 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 8.7/26.7 MB 2.2 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 9.2/26.7 MB 2.2 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 9.4/26.7 MB 2.2 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 10.0/26.7 MB 2.2 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 10.5/26.7 MB 2.2 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 11.0/26.7 MB 2.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.5/26.7 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 12.1/26.7 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 12.6/26.7 MB 2.2 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 13.1/26.7 MB 2.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 13.6/26.7 MB 2.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 14.2/26.7 MB 2.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 14.4/26.7 MB 2.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.9/26.7 MB 2.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 15.5/26.7 MB 2.2 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 16.0/26.7 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 16.5/26.7 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 17.0/26.7 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 17.6/26.7 MB 2.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 18.1/26.7 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 18.6/26.7 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.9/26.7 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 19.4/26.7 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 20.2/26.7 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 20.4/26.7 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 21.0/26.7 MB 2.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 21.5/26.7 MB 2.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 22.0/26.7 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 22.5/26.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 23.1/26.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.6/26.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 24.1/26.7 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 24.6/26.7 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.7 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.7/26.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.7/26.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.7/26.7 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading mlflow_skinny-2.17.2-py3-none-any.whl (5.7 MB)\n",
      "   ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.7 MB 2.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.7 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.8/5.7 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.4/5.7 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.9/5.7 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.4/5.7 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.9/5.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.5/5.7 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.7 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.0/5.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.7 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.7/5.7 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.2 MB 1.3 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 0.8/25.2 MB 1.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 1.3/25.2 MB 1.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 1.8/25.2 MB 1.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 2.1/25.2 MB 1.6 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 2.4/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 2.6/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 3.1/25.2 MB 1.7 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 3.7/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.6 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 4.2/25.2 MB 1.5 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 4.7/25.2 MB 1.6 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 5.2/25.2 MB 1.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 5.8/25.2 MB 1.7 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 6.3/25.2 MB 1.7 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 1.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 7.3/25.2 MB 1.8 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 7.6/25.2 MB 1.8 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 8.1/25.2 MB 1.8 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 8.7/25.2 MB 1.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 9.2/25.2 MB 1.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 9.4/25.2 MB 1.9 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 10.0/25.2 MB 1.9 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.7/25.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.3/25.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 11.5/25.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 12.1/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.6/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 13.1/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 13.6/25.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 13.9/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.9/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 15.5/25.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 15.7/25.2 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 16.3/25.2 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.8/25.2 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 17.3/25.2 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 17.8/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 19.1/25.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 19.7/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.7/25.2 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.2/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.8/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/25.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.3/25.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.9/25.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading sqlalchemy-2.0.40-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading databricks_sdk-0.50.0-py3-none-any.whl (692 kB)\n",
      "   ---------------------------------------- 0.0/692.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 262.1/692.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 692.3/692.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.1.1-cp38-cp38-win_amd64.whl (298 kB)\n",
      "Downloading opentelemetry_api-1.32.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.32.0-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl (188 kB)\n",
      "Downloading protobuf-5.29.4-cp38-cp38-win_amd64.whl (434 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl (102 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading wrapt-1.17.2-cp38-cp38-win_amd64.whl (38 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: wrapt, waitress, urllib3, sqlparse, smmap, pyasn1, pyarrow, protobuf, Mako, idna, greenlet, graphql-core, cloudpickle, charset-normalizer, certifi, cachetools, sqlalchemy, rsa, requests, pyasn1-modules, markdown, graphql-relay, gitdb, deprecated, opentelemetry-api, graphene, google-auth, gitpython, docker, alembic, opentelemetry-semantic-conventions, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed Mako-1.3.10 alembic-1.14.1 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpickle-3.1.1 databricks-sdk-0.50.0 deprecated-1.2.18 docker-7.1.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.39.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 greenlet-3.1.1 idna-3.10 markdown-3.7 mlflow-2.17.2 mlflow-skinny-2.17.2 opentelemetry-api-1.32.0 opentelemetry-sdk-1.32.0 opentelemetry-semantic-conventions-0.53b0 protobuf-5.29.4 pyarrow-17.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.3 rsa-4.9 smmap-5.0.2 sqlalchemy-2.0.40 sqlparse-0.5.3 urllib3-2.2.3 waitress-3.0.0 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13607e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mlflow\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://ec2-54-234-87-231.compute-1.amazonaws.com:5000/\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 15)\n",
    "    mlflow.log_metric(\"metric1\", 0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f684a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating baseline model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88110540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_comment  category\n",
       "0   family mormon have never tried explain them t...         1\n",
       "1  buddhism has very much lot compatible with chr...         1\n",
       "2  seriously don say thing first all they won get...        -1\n",
       "3  what you have learned yours and only yours wha...         0\n",
       "4  for your own benefit you may want read living ...         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bca6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24fa25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f6fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df['clean_comment'].str.strip() == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c356a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4192bdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HEART\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HEART\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc02331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_comment(comment):\n",
    "    # Convert to lowercase\n",
    "    comment = comment.lower()\n",
    "\n",
    "    # Remove trailing and leading whitespaces\n",
    "    comment = comment.strip()\n",
    "\n",
    "    # Remove newline characters\n",
    "    comment = re.sub(r'\\n', ' ', comment)\n",
    "\n",
    "    # Remove non-alphanumeric characters, except punctuation\n",
    "    comment = re.sub(r'[^A-Za-z0-9\\s!?.,]', '', comment)\n",
    "\n",
    "    # Remove stopwords but retain important ones for sentiment analysis\n",
    "    stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
    "    comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    comment = ' '.join([lemmatizer.lemmatize(word) for word in comment.split()])\n",
    "\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf02cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'clean_comment' column\n",
    "df['clean_comment'] = df['clean_comment'].apply(preprocess_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e71d502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon never tried explain still stare ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism much lot compatible christianity espe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously say thing first get complex explain ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learned want teach different focus goal not wr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benefit may want read living buddha living chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_comment  category\n",
       "0  family mormon never tried explain still stare ...         1\n",
       "1  buddhism much lot compatible christianity espe...         1\n",
       "2  seriously say thing first get complex explain ...        -1\n",
       "3  learned want teach different focus goal not wr...         0\n",
       "4  benefit may want read living buddha living chr...         1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae74600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a811df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Vectorize the comments using Bag of Words (CountVectorizer)\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Bag of Words model with a limit of 1000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8cb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['clean_comment']).toarray()\n",
    "y = df['category']  # Assuming 'sentiment' is the target variable (0 or 1 for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d2765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8899d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36793, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595e618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2       -1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "37244    0\n",
       "37245    1\n",
       "37246    0\n",
       "37247    1\n",
       "37248    0\n",
       "Name: category, Length: 36793, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f354f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36793,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a25820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up the MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://ec2-54-234-87-231.compute-1.amazonaws.com:5000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set or create an experiment\n",
    "mlflow.set_experiment(\"Ex-1: RF Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8446a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "575eb344",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HEART\\anaconda3\\envs\\MLPro\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 1: Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Define and train a Random Forest baseline model using a simple train-test split\n",
    "with mlflow.start_run() as run:\n",
    "    # Log a description for the run\n",
    "    mlflow.set_tag(\"mlflow.runName\", \"RandomForest_Baseline_TrainTestSplit\")\n",
    "    mlflow.set_tag(\"experiment_type\", \"baseline\")\n",
    "    mlflow.set_tag(\"model_type\", \"RandomForestClassifier\")\n",
    "\n",
    "    # Add a description\n",
    "    mlflow.set_tag(\"description\", \"Baseline RandomForest model for sentiment analysis using Bag of Words (BoW) with a simple train-test split\")\n",
    "\n",
    "    # Log parameters for the vectorizer\n",
    "    mlflow.log_param(\"vectorizer_type\", \"CountVectorizer\")\n",
    "    mlflow.log_param(\"vectorizer_max_features\", vectorizer.max_features)\n",
    "\n",
    "    # Log Random Forest parameters\n",
    "    n_estimators = 200\n",
    "    max_depth = 15\n",
    "\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Log metrics for each class and accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    for label, metrics in classification_rep.items():\n",
    "        if isinstance(metrics, dict):  # For precision, recall, f1-score, etc.\n",
    "            for metric, value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric}\", value)\n",
    "\n",
    "    # Confusion matrix plot\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "\n",
    "    # Save and log the confusion matrix plot\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"/content/confusion_matrix.png\")\n",
    "\n",
    "    # Log the Random Forest model\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "\n",
    "    # Optionally log the dataset itself (if it's small enough)\n",
    "    df.to_csv(\"dataset.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/content/dataset.csv\")\n",
    "\n",
    "# Display final accuracy\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef35224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reddit_preprocessing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('reddit_preprocessing.csv').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
